qsub_submitter	current qsub status	params	batch_script	total_hrs_run	max_epoch	latest_ckpoint_path	std_out	train_accuracy	val_accuracy	train_cmd	notes	
dg							std_out_decoder_1	0.941	0.842			
dg			bs_current	10	377	/projectnb/textconv/dgordon/state-spaces/outputs/2023-10-07/14-24-39-705466/checkpoints/val/accuracy.ckpt	std_out_		0.755	python -m train_debug_current pipeline=sc dataset=sc10 model=s4 model.n_layers=2 loader.eval_resolutions=1 trainer.max_epochs=500 train.ckpt=/projectnb/textconv/dgordon/state-spaces/outputs/2023-10-07/14-24-39-705466/checkpoints/val/accuracy.ckpt	"still improving.  Increased to 800 epochs, reducted lr to 0.0005.  lr=0.0003, ran again. Reduced to 0.0002, changed dir (that was the error, no w on the dir).  Swapping to warm up.  Checkpoints working."	
dg	qw	453k	bs_1				std_out_1		0.1019	python -m train_hydra pipeline=sc dataset=sc10 model=s4 model.n_layers=2 loader.eval_resolutions=1 tc.conv.segment_length=200 tc.conv.stride=100 tc.transformer.trans_dim=50.  reduced lr to 0.0005 tc.transformer.num_layers=4 tc.transformer.num_heads=10		
dg	qw	467k	bs_2				std_out_2		0.813	python -m train_hydra pipeline=sc dataset=sc10 optimizer.lr=0.0007 model=s4 model.n_layers=2 loader.eval_resolutions=1 loader.batch_size=10 scheduler=linear_warmup tc.conv.stride=20. lr to 0.0002 hydra.run.dir=./outputs/p2/${now:%H-%M-%S-%f}	"added warmup, put lr to 0.0007, just a little lower with the bs=10, changed lr to 0.0002.  reduced to 0.0001, switched to cosine warmup."	"81.3 after 49 epochs, and it looks like it is increasing slowly!! Running again with accuracy chkpt"
dg	DONE!	395k	bs_3	10	109	/projectnb/textconv/dgordon/state-spaces/outputs/2023-10-08/00-14-08-376551/checkpoints/val/accuracy.ckpt	std_out_3		0.10515	python -m train_hydra pipeline=sc dataset=sc10 model=s4 model.n_layers=2 loader.eval_resolutions=1 tc.transformer.trans_dim=20 tc.transformer.ff_dim=100 tc.transformer.num_layers=10 tc.transformer.num_heads=5	it doesn't look like it learns…I think I'll try a higher lr for a bit? Trying lower lr=0.0002. outputs perm.  Hydra.run.dir.	
dg	DONE!	1.2M	bs_4				std_out_4		0.254	python -m train_hydra pipeline=sc dataset=sc10 model=s4 model.n_layers=2 model.d_model=512 loader.eval_resolutions=1 tc.transformer.trans_dim=20 tc.transformer.ff_dim=100 tc.transformer.num_layers=10 tc.transformer.num_heads=5	"nothing after 90 epochs.  Reducing lr, scheduling for 10 hrs. (20 hors got to like 90 epochs). Started learning at 11 epochs (lr=0.0002), Plateaued after 16. going to load and increase rate to 0.0005. can't load after adding scheduler.  Removed and loaded with increased lr."	nothing.  Adding cos and lr math
dg	r	693k	bs_5lrplus				std_out_5lrplus		0.648	python -m train_hydra pipeline=sc dataset=sc10 model=s4 model.n_layers=4 loader.eval_resolutions=1 tc.transformer.trans_dim=20 tc.transformer.ff_dim=100 tc.transformer.num_layers=10 tc.transformer.num_heads=5	"outputs permission denied.  Changing hydra.run.dir.  Fixed lr and batch size. Ran to epoch 24.  added checkpoint. Added warmup.  Now it is training after 8 epochs.  Ffdim and transformer num layers are both double the default.  Learning rate was way lower too, 0.0002, normal is 0.001.  loading and increasing lr to 0.0004.  removed scheduler.  not sure that will load correctly..."	it loaded.   And trained.  Got to 64%.  Plateaued.  Adding cosine.  Increased lr.  
dg	qw		bs_6				std_out_6		0.347	python -m train_hydra pipeline=sc dataset=sc10 model=s4 model.n_layers=2 loader.eval_resolutions=1 tc.conv.segment_length=500 tc.conv.stride=50	"og request was for 4 A40s, not scheduled in 12 hours, reducing. Oomed with 2.  reduced to batchsize 2, and reduced lr. Oom again.  Bs_1.  warmup!.  Nothing after 8 epochs, reducing lr to lr=0.00009. trained fast! Got to 25% first epoch, then stuck at 34%. loading and upping learning rate (starting with 0.0001)."	added plateau.  Trying that this time.  Fixed val/accuracy key for monitor.
dg			bs_7				std_out_7			python -m train_hydra pipeline=sc dataset=sc10 model=s4 model.n_layers=2 loader.eval_resolutions=1 model.bidirectional=true	"og request was for 4 A40s, not scheduled in 12 hours, reducing."	
gk			bs_8				std_out_8			python -m train_hydra pipeline=sc dataset=sc10 model=s4 model.n_layers=2 loader.eval_resolutions=1 tc.conv.stride=20 model.bidirectional=true		
dg	DONE!		bs_9				std_out_9		0.848	python -m train_hydra pipeline=sc dataset=sc10 model=s4 model.n_layers=6 model.d_model=512 loader.eval_resolutions=1	"nothing after 20 epochs, reducing lr lr=0.0004.  down to 0.0002.  switched to cosine. Woah.  This got high fast.  Scheduler cosine yo.  That was with a batch of 10 too. "	"seems to have maxed out.  Also, I used scheduler=cosine and a lr of 0.0002 and batch 10.  I think that is the thing I want to try.  Batch and lr and cosine.  Notice batch was 50/5 and lr is 0.001/5.  try that. "
dg	r		bs_10				std_out_10			python -m train_hydra pipeline=sc dataset=sc10 model=s4 model.n_layers=2 loader.eval_resolutions=1 tc.conv.segment_length=400 tc.conv.stride=400 tc.transformer.trans_dim=100 tc.transformer.ff_dim=100 tc.transformer.num_heads=20	"it is a big model, reduced to bs 5, lr to 0.0001"	
v			bs_11				std_out_11			python -m train_hydra pipeline=sc dataset=sc10 model=s4 model.n_layers=4 model.d_model=128 loader.eval_resolutions=1 tc.conv.segment_length=400 tc.conv.stride=400 tc.transformer.trans_dim=100 tc.transformer.ff_dim=100 tc.transformer.num_heads=20		
v			bs_12				std_out_12			python -m train_hydra pipeline=sc dataset=sc10 model=s4 model.n_layers=6 model.d_model=128 loader.eval_resolutions=1 tc.conv.segment_length=200 tc.conv.stride=50 tc.transformer.trans_dim=80 tc.transformer.num_layers=10 tc.transformer.num_heads=10		
v			bs_13				std_out_13			python -m train_hydra pipeline=sc dataset=sc10 model=s4 model.n_layers=4 model.d_model=512 loader.eval_resolutions=1 tc.conv.segment_length=400 tc.conv.stride=20 tc.transformer.trans_dim=40 tc.transformer.num_layers=8 tc.transformer.num_heads=8		
v			bs_14				std_out_14			python -m train_hydra pipeline=sc dataset=sc10 model=s4 model.n_layers=4 loader.eval_resolutions=1 	this is just the original model with our defaults.  	
gk			bs_15				std_out_15			python -m train_hydra pipeline=sc dataset=sc10 model=s4 model.n_layers=4 loader.batch_size=10 model.d_model=128 loader.eval_resolutions=1 tc.conv.segment_length=400 tc.conv.stride=50		
gk			bs_16				std_out_16			python -m train_hydra pipeline=sc dataset=sc10 model=s4 model.n_layers=4 loader.batch_size=10 model.d_model=128 loader.eval_resolutions=1 tc.conv.segment_length=400 tc.conv.stride=10		
gk			bs_17				std_out_17			python -m train_hydra pipeline=sc dataset=sc10 model=s4 model.n_layers=4 model.d_model=512 optimizer.lr=0.0002 loader.batch_size=5 loader.eval_resolutions=1 tc.conv.segment_length=500 tc.conv.stride=20 tc.transformer.trans_dim=40 tc.transformer.num_heads=10		
dg			bs_hydra_default_test				std_out_hydra_default_test	0.345	0.334	This is my test to make sure my hydra defaults run the same as the 84% we got.  	"only ran 10 hours, got to epoch 124.  im satisfied.  "	
dg			bs_init_test							"will throw.  First test is to make sure the init as I wrote it works.  Qsubbing since the scc is slow. Outputs permissions….fixed dir.  4 hours, 100 epochs.  "		
dg			bs_init_lr				std_out_init_lr			lr=0.01	nothing.  Messing with scheduling instead	
dg	DONE!		bs_warmup					0.96	0.88!!!!!!!!!	default with scheduler=linear_warmup		
dg	DONE!		bs_warmupadam					0.958	0.882	also with adam.  		
dg	DONE!	1.2M	bs_4_b				std_out_4b		0.254	python -m train_hydra pipeline=sc dataset=sc10 model=s4 model.n_layers=2 model.d_model=512 loader.eval_resolutions=1 tc.transformer.trans_dim=20 tc.transformer.ff_dim=100 tc.transformer.num_layers=10 tc.transformer.num_heads=5	"nothing after 90 epochs.  Reducing lr, scheduling for 10 hrs. (20 hors got to like 90 epochs). Started learning at 11 epochs (lr=0.0002), Plateaued after 16. increased to 0.0004"	no.  Scheduler mama
dg	DONE!	693k	bs_5_b				std_out_5		0.105	python -m train_hydra pipeline=sc dataset=sc10 model=s4 model.n_layers=4 loader.eval_resolutions=1 tc.transformer.trans_dim=20 tc.transformer.ff_dim=100 tc.transformer.num_layers=10 tc.transformer.num_heads=5	"outputs permission denied.  Changing hydra.run.dir.  Fixed lr and batch size. Ran to epoch 24.  added checkpoint. Added warmup.  Now it is training after 8 epochs.  Ffdim and transformer num layers are both double the default.  Learning rate was way lower too, 0.0002, normal is 0.001. swapped to lr 0.0003 and cosine warmup. "	
dg	qw		bs_5_cos				std_out_5_cos			standardized and reran.  	except I added scheduler=cosine.	
dg	qw		bs_5_timm_cosine							5 with timm cosine (cosine and restart).  Messed  up the config.  Resubmitted.		
dg	r		bs_4_tcos							4 with tcos scheduler (cos with restarts) also adjusted lr to work with the batch size (learned from bs_9)	made a scheduler called tcos that does 10 epoch cosine cycles.	
dg	r		bs_3_cos							"bs_3 with lr, bs, and scheduler set."		
dg	qw		bs_4_cos				std_out_4_cos				"4 but with lr=0.0002, batch 10, cosine scheduler"	
dg	qw		bs_2_cos							"bs_2 had lr 0.0001 with cosine warmup.  This is cosine (no warmup) with lr 0.0002 and bs 10.  bs_2 looks to be learning slow there at the end, so yeah, this seemed like a good test"		
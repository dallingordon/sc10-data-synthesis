qsub_submitter	current qsub status	params	batch_script	total_hrs_run	max_epoch	latest_ckpoint_path	std_out	train_accuracy	val_accuracy	train_cmd	notes
dg							std_out_decoder_1	0.941	0.842		
dg	qw		bs_current	10	377	/projectnb/textconv/dgordon/state-spaces/outputs/2023-10-07/14-24-39-705466/checkpoints/val/accuracy.ckpt	std_out_		73.25	python -m train_debug_current pipeline=sc dataset=sc10 model=s4 model.n_layers=2 loader.eval_resolutions=1 trainer.max_epochs=500 train.ckpt=/projectnb/textconv/dgordon/state-spaces/outputs/2023-10-07/14-24-39-705466/checkpoints/val/accuracy.ckpt	"still improving.  Increased to 800 epochs, reducted lr to 0.0005.  lr=0.0003, ran again. Reduced to 0.0002, changed dir (that was the error, no w on the dir)"
dg		453k	bs_1				std_out_1		0.1019	python -m train_hydra pipeline=sc dataset=sc10 model=s4 model.n_layers=2 loader.eval_resolutions=1 tc.conv.segment_length=200 tc.conv.stride=100 tc.transformer.trans_dim=50 tc.transformer.num_layers=4 tc.transformer.num_heads=10	
dg	qw	467k	bs_2				std_out_2			python -m train_hydra pipeline=sc dataset=sc10 model=s4 model.n_layers=2 loader.eval_resolutions=1 tc.conv.stride=20	"oom.  Changed to batch_size 10.  upped lr to 0.005 (hasn't learned, and only got 30 ish epochs).  Now trying low lr, 0.0002.  see if that learns with tiny batch. Just outputs perm"
dg	qw	395k	bs_3	10	109	/projectnb/textconv/dgordon/state-spaces/outputs/2023-10-08/00-14-08-376551/checkpoints/val/accuracy.ckpt	std_out_3		0.10515	python -m train_hydra pipeline=sc dataset=sc10 model=s4 model.n_layers=2 loader.eval_resolutions=1 tc.transformer.trans_dim=20 tc.transformer.ff_dim=100 tc.transformer.num_layers=10 tc.transformer.num_heads=5	it doesn't look like it learns…I think I'll try a higher lr for a bit? Trying lower lr=0.0002. outputs perm.  Hydra.run.dir.
dg	r	1.2M	bs_4				std_out_4			python -m train_hydra pipeline=sc dataset=sc10 model=s4 model.n_layers=2 model.d_model=512 loader.eval_resolutions=1 tc.transformer.trans_dim=20 tc.transformer.ff_dim=100 tc.transformer.num_layers=10 tc.transformer.num_heads=5	"reduced to 2 A40s.  Oom, reducing batch to 10. also doing the output directory with project name here.  Reduced to 5 hours"
dg	qw	693k	bs_5				std_out_5			python -m train_hydra pipeline=sc dataset=sc10 model=s4 model.n_layers=4 loader.eval_resolutions=1 tc.transformer.trans_dim=20 tc.transformer.ff_dim=100 tc.transformer.num_layers=10 tc.transformer.num_heads=5	outputs permission denied.  Changing hydra.run.dir.  Fixed lr and batch size.
dg	qw		bs_6				std_out_6			python -m train_hydra pipeline=sc dataset=sc10 model=s4 model.n_layers=2 loader.eval_resolutions=1 tc.conv.segment_length=500 tc.conv.stride=50	"og request was for 4 A40s, not scheduled in 12 hours, reducing. Oomed with 2.  reduced to batchsize 2, and reduced lr."
dg			bs_7				std_out_7			python -m train_hydra pipeline=sc dataset=sc10 model=s4 model.n_layers=2 loader.eval_resolutions=1 model.bidirectional=true	"og request was for 4 A40s, not scheduled in 12 hours, reducing."
gk			bs_8				std_out_8			python -m train_hydra pipeline=sc dataset=sc10 model=s4 model.n_layers=2 loader.eval_resolutions=1 tc.conv.stride=20 model.bidirectional=true	
			bs_9				std_out_9			python -m train_hydra pipeline=sc dataset=sc10 model=s4 model.n_layers=6 model.d_model=512 loader.eval_resolutions=1	
			bs_10				std_out_10			python -m train_hydra pipeline=sc dataset=sc10 model=s4 model.n_layers=2 loader.eval_resolutions=1 tc.conv.segment_length=400 tc.conv.stride=400 tc.transformer.trans_dim=100 tc.transformer.ff_dim=100 tc.transformer.num_heads=20	
			bs_11				std_out_11			python -m train_hydra pipeline=sc dataset=sc10 model=s4 model.n_layers=4 model.d_model=128 loader.eval_resolutions=1 tc.conv.segment_length=400 tc.conv.stride=400 tc.transformer.trans_dim=100 tc.transformer.ff_dim=100 tc.transformer.num_heads=20	
			bs_12				std_out_12			python -m train_hydra pipeline=sc dataset=sc10 model=s4 model.n_layers=6 model.d_model=128 loader.eval_resolutions=1 tc.conv.segment_length=200 tc.conv.stride=50 tc.transformer.trans_dim=80 tc.transformer.num_layers=10 tc.transformer.num_heads=10	
			bs_13				std_out_13			python -m train_hydra pipeline=sc dataset=sc10 model=s4 model.n_layers=4 model.d_model=512 loader.eval_resolutions=1 tc.conv.segment_length=400 tc.conv.stride=20 tc.transformer.trans_dim=40 tc.transformer.num_layers=8 tc.transformer.num_heads=8	
			bs_14				std_out_14			python -m train_hydra pipeline=sc dataset=sc10 model=s4 model.n_layers=4 loader.eval_resolutions=1 	this is just the original model with our defaults.  
			bs_15				std_out_15			python -m train_hydra pipeline=sc dataset=sc10 model=s4 model.n_layers=4 loader.batch_size=10 model.d_model=128 loader.eval_resolutions=1 tc.conv.segment_length=400 tc.conv.stride=50	
			bs_16				std_out_16			python -m train_hydra pipeline=sc dataset=sc10 model=s4 model.n_layers=4 loader.batch_size=10 model.d_model=128 loader.eval_resolutions=1 tc.conv.segment_length=400 tc.conv.stride=10	
			bs_17				std_out_17			python -m train_hydra pipeline=sc dataset=sc10 model=s4 model.n_layers=4 model.d_model=512 optimizer.lr=0.0002 loader.batch_size=5 loader.eval_resolutions=1 tc.conv.segment_length=500 tc.conv.stride=20 tc.transformer.trans_dim=40 tc.transformer.num_heads=10	
			bs_hydra_default_test				std_out_hydra_default_test	0.345	0.334	This is my test to make sure my hydra defaults run the same as the 84% we got.  	"only ran 10 hours, got to epoch 124.  im satisfied.  "
dg	r		bs_init_test							"will throw.  First test is to make sure the init as I wrote it works.  Qsubbing since the scc is slow. Outputs permissions….fixed dir.  4 hours, 100 epochs.  "	
dg			bs_init_lr				std_out_init_lr			lr=0.01	nothing.  Messing with scheduling instead
dg	qw		bs_warmup							default with scheduler=linear_warmup	
dg	qw		bs_warmupadam							also with adam.  	
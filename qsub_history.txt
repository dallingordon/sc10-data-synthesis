qsub_submitter	current qsub status	params	batch_script	total_hrs_run	max_epoch	latest_ckpoint_path	std_out	train_accuracy	val_accuracy	train_cmd	notes
dg							std_out_decoder_1	0.941	0.842		
dg			bs_current	10	377	/projectnb/textconv/dgordon/state-spaces/outputs/2023-10-07/14-24-39-705466/checkpoints/val/accuracy.ckpt	std_out_		0.755	python -m train_debug_current pipeline=sc dataset=sc10 model=s4 model.n_layers=2 loader.eval_resolutions=1 trainer.max_epochs=500 train.ckpt=/projectnb/textconv/dgordon/state-spaces/outputs/2023-10-07/14-24-39-705466/checkpoints/val/accuracy.ckpt	"still improving.  Increased to 800 epochs, reducted lr to 0.0005.  lr=0.0003, ran again. Reduced to 0.0002, changed dir (that was the error, no w on the dir).  Swapping to warm up.  Checkpoints working."
dg	qw	453k	bs_1				std_out_1		0.1019	python -m train_hydra pipeline=sc dataset=sc10 model=s4 model.n_layers=2 loader.eval_resolutions=1 tc.conv.segment_length=200 tc.conv.stride=100 tc.transformer.trans_dim=50.  reduced lr to 0.0005 tc.transformer.num_layers=4 tc.transformer.num_heads=10	
dg	qw	467k	bs_2				std_out_2			python -m train_hydra pipeline=sc dataset=sc10 optimizer.lr=0.0007 model=s4 model.n_layers=2 loader.eval_resolutions=1 loader.batch_size=10 scheduler=linear_warmup tc.conv.stride=20. lr to 0.0002 hydra.run.dir=./outputs/p2/${now:%H-%M-%S-%f}	"added warmup, put lr to 0.0007, just a little lower with the bs=10, changed lr to 0.0002.  "
dg	DONE!	395k	bs_3	10	109	/projectnb/textconv/dgordon/state-spaces/outputs/2023-10-08/00-14-08-376551/checkpoints/val/accuracy.ckpt	std_out_3		0.10515	python -m train_hydra pipeline=sc dataset=sc10 model=s4 model.n_layers=2 loader.eval_resolutions=1 tc.transformer.trans_dim=20 tc.transformer.ff_dim=100 tc.transformer.num_layers=10 tc.transformer.num_heads=5	it doesn't look like it learns…I think I'll try a higher lr for a bit? Trying lower lr=0.0002. outputs perm.  Hydra.run.dir.
dg	qw	1.2M	bs_4				std_out_4		0.254	python -m train_hydra pipeline=sc dataset=sc10 model=s4 model.n_layers=2 model.d_model=512 loader.eval_resolutions=1 tc.transformer.trans_dim=20 tc.transformer.ff_dim=100 tc.transformer.num_layers=10 tc.transformer.num_heads=5	"nothing after 90 epochs.  Reducing lr, scheduling for 10 hrs. (20 hors got to like 90 epochs). Started learning at 11 epochs (lr=0.0002), Plateaued after 16. going to load and increase rate to 0.0005. 4b starts over with higher."
dg	r	693k	bs_5				std_out_5		0.452	python -m train_hydra pipeline=sc dataset=sc10 model=s4 model.n_layers=4 loader.eval_resolutions=1 tc.transformer.trans_dim=20 tc.transformer.ff_dim=100 tc.transformer.num_layers=10 tc.transformer.num_heads=5	"outputs permission denied.  Changing hydra.run.dir.  Fixed lr and batch size. Ran to epoch 24.  added checkpoint. Added warmup.  Now it is training after 8 epochs.  Ffdim and transformer num layers are both double the default.  Learning rate was way lower too, 0.0002, normal is 0.001.  loading and increasing lr to 0.0004.  removed scheduler.  not sure that will load correctly..."
dg	qw		bs_6				std_out_6			python -m train_hydra pipeline=sc dataset=sc10 model=s4 model.n_layers=2 loader.eval_resolutions=1 tc.conv.segment_length=500 tc.conv.stride=50	"og request was for 4 A40s, not scheduled in 12 hours, reducing. Oomed with 2.  reduced to batchsize 2, and reduced lr. Oom again.  Bs_1.  warmup!.  Nothing after 8 epochs, reducing lr to lr=0.00009"
dg			bs_7				std_out_7			python -m train_hydra pipeline=sc dataset=sc10 model=s4 model.n_layers=2 loader.eval_resolutions=1 model.bidirectional=true	"og request was for 4 A40s, not scheduled in 12 hours, reducing."
gk			bs_8				std_out_8			python -m train_hydra pipeline=sc dataset=sc10 model=s4 model.n_layers=2 loader.eval_resolutions=1 tc.conv.stride=20 model.bidirectional=true	
dg	qw		bs_9				std_out_9			python -m train_hydra pipeline=sc dataset=sc10 model=s4 model.n_layers=6 model.d_model=512 loader.eval_resolutions=1	"nothing after 20 epochs, reducing lr lr=0.0004"
dg	qw		bs_10				std_out_10			python -m train_hydra pipeline=sc dataset=sc10 model=s4 model.n_layers=2 loader.eval_resolutions=1 tc.conv.segment_length=400 tc.conv.stride=400 tc.transformer.trans_dim=100 tc.transformer.ff_dim=100 tc.transformer.num_heads=20	"it is a big model, reduced to bs 5, lr to 0.0001"
			bs_11				std_out_11			python -m train_hydra pipeline=sc dataset=sc10 model=s4 model.n_layers=4 model.d_model=128 loader.eval_resolutions=1 tc.conv.segment_length=400 tc.conv.stride=400 tc.transformer.trans_dim=100 tc.transformer.ff_dim=100 tc.transformer.num_heads=20	
			bs_12				std_out_12			python -m train_hydra pipeline=sc dataset=sc10 model=s4 model.n_layers=6 model.d_model=128 loader.eval_resolutions=1 tc.conv.segment_length=200 tc.conv.stride=50 tc.transformer.trans_dim=80 tc.transformer.num_layers=10 tc.transformer.num_heads=10	
			bs_13				std_out_13			python -m train_hydra pipeline=sc dataset=sc10 model=s4 model.n_layers=4 model.d_model=512 loader.eval_resolutions=1 tc.conv.segment_length=400 tc.conv.stride=20 tc.transformer.trans_dim=40 tc.transformer.num_layers=8 tc.transformer.num_heads=8	
			bs_14				std_out_14			python -m train_hydra pipeline=sc dataset=sc10 model=s4 model.n_layers=4 loader.eval_resolutions=1 	this is just the original model with our defaults.  
			bs_15				std_out_15			python -m train_hydra pipeline=sc dataset=sc10 model=s4 model.n_layers=4 loader.batch_size=10 model.d_model=128 loader.eval_resolutions=1 tc.conv.segment_length=400 tc.conv.stride=50	
			bs_16				std_out_16			python -m train_hydra pipeline=sc dataset=sc10 model=s4 model.n_layers=4 loader.batch_size=10 model.d_model=128 loader.eval_resolutions=1 tc.conv.segment_length=400 tc.conv.stride=10	
			bs_17				std_out_17			python -m train_hydra pipeline=sc dataset=sc10 model=s4 model.n_layers=4 model.d_model=512 optimizer.lr=0.0002 loader.batch_size=5 loader.eval_resolutions=1 tc.conv.segment_length=500 tc.conv.stride=20 tc.transformer.trans_dim=40 tc.transformer.num_heads=10	
			bs_hydra_default_test				std_out_hydra_default_test	0.345	0.334	This is my test to make sure my hydra defaults run the same as the 84% we got.  	"only ran 10 hours, got to epoch 124.  im satisfied.  "
dg			bs_init_test							"will throw.  First test is to make sure the init as I wrote it works.  Qsubbing since the scc is slow. Outputs permissions….fixed dir.  4 hours, 100 epochs.  "	
dg			bs_init_lr				std_out_init_lr			lr=0.01	nothing.  Messing with scheduling instead
dg	DONE!		bs_warmup					0.96	0.88!!!!!!!!!	default with scheduler=linear_warmup	
dg	DONE!		bs_warmupadam					0.958	0.882	also with adam.  	
dg	qw	1.2M	bs_4_b				std_out_4b		0.254	python -m train_hydra pipeline=sc dataset=sc10 model=s4 model.n_layers=2 model.d_model=512 loader.eval_resolutions=1 tc.transformer.trans_dim=20 tc.transformer.ff_dim=100 tc.transformer.num_layers=10 tc.transformer.num_heads=5	"nothing after 90 epochs.  Reducing lr, scheduling for 10 hrs. (20 hors got to like 90 epochs). Started learning at 11 epochs (lr=0.0002), Plateaued after 16. increased to 0.0004"
dg	r	693k	bs_5_b				std_out_5		0.105	python -m train_hydra pipeline=sc dataset=sc10 model=s4 model.n_layers=4 loader.eval_resolutions=1 tc.transformer.trans_dim=20 tc.transformer.ff_dim=100 tc.transformer.num_layers=10 tc.transformer.num_heads=5	"outputs permission denied.  Changing hydra.run.dir.  Fixed lr and batch size. Ran to epoch 24.  added checkpoint. Added warmup.  Now it is training after 8 epochs.  Ffdim and transformer num layers are both double the default.  Learning rate was way lower too, 0.0002, normal is 0.001"
									0.452	python -m train_hydra pipeline=sc dataset=sc10 model=s4 model.n_layers=4 loader.eval_resolutions=1 tc.transformer.trans_dim=20 tc.transformer.ff_dim=100 tc.transformer.num_layers=10 tc.transformer.num_heads=5	increased lr to 0.0004
											
											
											
											
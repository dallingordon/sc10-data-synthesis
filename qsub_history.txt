qsub_submitter	current qsub status	params	batch_script	total_hrs_run	max_epoch	latest_ckpoint_path	std_out	train_accuracy	val_accuracy	train_cmd	notes		Notes			"schedulers: cosine, cosine_warmup, tcos (cosine with restarts)"
dg							std_out_decoder_1	0.941	0.842				"it seems that increasing s4 params is not the way.  Std out 9 maxes out at 84 with double the hidden dim, and more layers.  However, keep batchsize and lr proportional 50:0.001, cosine scheduler works nice.  Messing with tcoss (cosine with restarts)"			
dg			bs_current	10	377	/projectnb/textconv/dgordon/state-spaces/outputs/2023-10-07/14-24-39-705466/checkpoints/val/accuracy.ckpt	std_out_		0.755	python -m train_debug_current pipeline=sc dataset=sc10 model=s4 model.n_layers=2 loader.eval_resolutions=1 trainer.max_epochs=500 train.ckpt=/projectnb/textconv/dgordon/state-spaces/outputs/2023-10-07/14-24-39-705466/checkpoints/val/accuracy.ckpt	"still improving.  Increased to 800 epochs, reducted lr to 0.0005.  lr=0.0003, ran again. Reduced to 0.0002, changed dir (that was the error, no w on the dir).  Swapping to warm up.  Checkpoints working."					
dg	qw	453k	bs_1				std_out_1		0.1019	python -m train_hydra pipeline=sc dataset=sc10 model=s4 model.n_layers=2 loader.eval_resolutions=1 tc.conv.segment_length=200 tc.conv.stride=100 tc.transformer.trans_dim=50.  reduced lr to 0.0005 tc.transformer.num_layers=4 tc.transformer.num_heads=10						
dg	DONE!	467k	bs_2				std_out_2		0.853	python -m train_hydra pipeline=sc dataset=sc10 optimizer.lr=0.0007 model=s4 model.n_layers=2 loader.eval_resolutions=1 loader.batch_size=10 scheduler=linear_warmup tc.conv.stride=20. lr to 0.0002 hydra.run.dir=./outputs/p2/${now:%H-%M-%S-%f}	"added warmup, put lr to 0.0007, just a little lower with the bs=10, changed lr to 0.0002.  reduced to 0.0001, switched to cosine warmup."	"81.3 after 49 epochs, and it looks like it is increasing slowly!! Running again with accuracy chkpt"	"with cosine warmup this is still increasing.  Slow, but increasing well still.  At epoch 74.  resubmitting."			
dg	DONE!	395k	bs_3	10	109	/projectnb/textconv/dgordon/state-spaces/outputs/2023-10-08/00-14-08-376551/checkpoints/val/accuracy.ckpt	std_out_3		0.10515	python -m train_hydra pipeline=sc dataset=sc10 model=s4 model.n_layers=2 loader.eval_resolutions=1 tc.transformer.trans_dim=20 tc.transformer.ff_dim=100 tc.transformer.num_layers=10 tc.transformer.num_heads=5	it doesn't look like it learns…I think I'll try a higher lr for a bit? Trying lower lr=0.0002. outputs perm.  Hydra.run.dir.					
dg	DONE!	1.2M	bs_4				std_out_4		0.254	python -m train_hydra pipeline=sc dataset=sc10 model=s4 model.n_layers=2 model.d_model=512 loader.eval_resolutions=1 tc.transformer.trans_dim=20 tc.transformer.ff_dim=100 tc.transformer.num_layers=10 tc.transformer.num_heads=5	"nothing after 90 epochs.  Reducing lr, scheduling for 10 hrs. (20 hors got to like 90 epochs). Started learning at 11 epochs (lr=0.0002), Plateaued after 16. going to load and increase rate to 0.0005. can't load after adding scheduler.  Removed and loaded with increased lr."	nothing.  Adding cos and lr math				
dg	DONE!	693k	bs_5lrplus				std_out_5lrplus		0.74	python -m train_hydra pipeline=sc dataset=sc10 model=s4 model.n_layers=4 loader.eval_resolutions=1 tc.transformer.trans_dim=20 tc.transformer.ff_dim=100 tc.transformer.num_layers=10 tc.transformer.num_heads=5	"outputs permission denied.  Changing hydra.run.dir.  Fixed lr and batch size. Ran to epoch 24.  added checkpoint. Added warmup.  Now it is training after 8 epochs.  Ffdim and transformer num layers are both double the default.  Learning rate was way lower too, 0.0002, normal is 0.001.  loading and increasing lr to 0.0004.  removed scheduler.  not sure that will load correctly..."	"it loaded.   And trained.  Got to 64%.  Plateaued.  Adding cosine.  Increased lr.  And it learns, but very slow.  This was at 0.0008.  I think scheduler.  5tcos is more promising.."				
dg	DONE!		bs_6				std_out_6		0.347	python -m train_hydra pipeline=sc dataset=sc10 model=s4 model.n_layers=2 loader.eval_resolutions=1 tc.conv.segment_length=500 tc.conv.stride=50	"og request was for 4 A40s, not scheduled in 12 hours, reducing. Oomed with 2.  reduced to batchsize 2, and reduced lr. Oom again.  Bs_1.  warmup!.  Nothing after 8 epochs, reducing lr to lr=0.00009. trained fast! Got to 25% first epoch, then stuck at 34%. loading and upping learning rate (starting with 0.0001)."	added plateau.  Trying that this time.  Fixed val/accuracy key for monitor.				
dg			bs_7				std_out_7			python -m train_hydra pipeline=sc dataset=sc10 model=s4 model.n_layers=2 loader.eval_resolutions=1 model.bidirectional=true	"og request was for 4 A40s, not scheduled in 12 hours, reducing."					
gk1			bs_8				std_out_8			python -m train_hydra pipeline=sc dataset=sc10 model=s4 model.n_layers=2 loader.eval_resolutions=1 tc.conv.stride=20 model.bidirectional=true						
dg	DONE!		bs_9				std_out_9		0.848	python -m train_hydra pipeline=sc dataset=sc10 model=s4 model.n_layers=6 model.d_model=512 loader.eval_resolutions=1	"nothing after 20 epochs, reducing lr lr=0.0004.  down to 0.0002.  switched to cosine. Woah.  This got high fast.  Scheduler cosine yo.  That was with a batch of 10 too. "	"seems to have maxed out.  Also, I used scheduler=cosine and a lr of 0.0002 and batch 10.  I think that is the thing I want to try.  Batch and lr and cosine.  Notice batch was 50/5 and lr is 0.001/5.  try that. "				
dg	DONE!		bs_10				std_out_10		0.85	python -m train_hydra pipeline=sc dataset=sc10 model=s4 model.n_layers=2 loader.eval_resolutions=1 tc.conv.segment_length=400 tc.conv.stride=400 tc.transformer.trans_dim=100 tc.transformer.ff_dim=100 tc.transformer.num_heads=20	"it is a big model, reduced to bs 5, lr to 0.0001 at epoch 7.  "	"okay, this was a larger input but with the stride at the same (400 and 400).  It got to 85 in 7 epochs with a batchsize of 5 (this was linear warmup) mmm, try with tcos, this could be a local min.  "				
v			bs_11				std_out_11			python -m train_hydra pipeline=sc dataset=sc10 model=s4 model.n_layers=4 model.d_model=128 loader.eval_resolutions=1 tc.conv.segment_length=400 tc.conv.stride=400 tc.transformer.trans_dim=100 tc.transformer.ff_dim=100 tc.transformer.num_heads=20						
v			bs_12				std_out_12			python -m train_hydra pipeline=sc dataset=sc10 model=s4 model.n_layers=6 model.d_model=128 loader.eval_resolutions=1 tc.conv.segment_length=200 tc.conv.stride=50 tc.transformer.trans_dim=80 tc.transformer.num_layers=10 tc.transformer.num_heads=10						
v			bs_13				std_out_13			python -m train_hydra pipeline=sc dataset=sc10 model=s4 model.n_layers=4 model.d_model=512 loader.eval_resolutions=1 tc.conv.segment_length=400 tc.conv.stride=20 tc.transformer.trans_dim=40 tc.transformer.num_layers=8 tc.transformer.num_heads=8						
v			bs_14				std_out_14			python -m train_hydra pipeline=sc dataset=sc10 model=s4 model.n_layers=4 loader.eval_resolutions=1 	this is just the original model with our defaults.  					
gk			bs_15				std_out_15			python -m train_hydra pipeline=sc dataset=sc10 model=s4 model.n_layers=4 loader.batch_size=10 model.d_model=128 loader.eval_resolutions=1 tc.conv.segment_length=400 tc.conv.stride=50						
gk			bs_16				std_out_16			python -m train_hydra pipeline=sc dataset=sc10 model=s4 model.n_layers=4 loader.batch_size=10 model.d_model=128 loader.eval_resolutions=1 tc.conv.segment_length=400 tc.conv.stride=10						
gk			bs_17				std_out_17			python -m train_hydra pipeline=sc dataset=sc10 model=s4 model.n_layers=4 model.d_model=512 optimizer.lr=0.0002 loader.batch_size=5 loader.eval_resolutions=1 tc.conv.segment_length=500 tc.conv.stride=20 tc.transformer.trans_dim=40 tc.transformer.num_heads=10						
dg			bs_hydra_default_test				std_out_hydra_default_test	0.345	0.334	This is my test to make sure my hydra defaults run the same as the 84% we got.  	"only ran 10 hours, got to epoch 124.  im satisfied.  "					
dg			bs_init_test							"will throw.  First test is to make sure the init as I wrote it works.  Qsubbing since the scc is slow. Outputs permissions….fixed dir.  4 hours, 100 epochs.  "						
dg			bs_init_lr				std_out_init_lr			lr=0.01	nothing.  Messing with scheduling instead					
dg	DONE!		bs_warmup					0.96	0.88!!!!!!!!!	default with scheduler=linear_warmup						
dg	DONE!		bs_warmupadam					0.958	0.882	also with adam.  						
dg	DONE!	1.2M	bs_4_b				std_out_4b		0.254	python -m train_hydra pipeline=sc dataset=sc10 model=s4 model.n_layers=2 model.d_model=512 loader.eval_resolutions=1 tc.transformer.trans_dim=20 tc.transformer.ff_dim=100 tc.transformer.num_layers=10 tc.transformer.num_heads=5	"nothing after 90 epochs.  Reducing lr, scheduling for 10 hrs. (20 hors got to like 90 epochs). Started learning at 11 epochs (lr=0.0002), Plateaued after 16. increased to 0.0004"	no.  Scheduler mama				
dg	DONE!	693k	bs_5_b				std_out_5		0.105	python -m train_hydra pipeline=sc dataset=sc10 model=s4 model.n_layers=4 loader.eval_resolutions=1 tc.transformer.trans_dim=20 tc.transformer.ff_dim=100 tc.transformer.num_layers=10 tc.transformer.num_heads=5	"outputs permission denied.  Changing hydra.run.dir.  Fixed lr and batch size. Ran to epoch 24.  added checkpoint. Added warmup.  Now it is training after 8 epochs.  Ffdim and transformer num layers are both double the default.  Learning rate was way lower too, 0.0002, normal is 0.001. swapped to lr 0.0003 and cosine warmup. "					
dg	DONE!		bs_5_cos				std_out_5_cos		0.845	standardized and reran.  	except I added scheduler=cosine.		more heads in the transformer!			
dg	qw		bs_5_timm_cosine							5 with timm cosine (cosine and restart).  Messed  up the config.  Resubmitted. Reduced lr and resubmitted.  10 hrs this time.						
dg	DONE!		bs_4_tcos					0.92	0.936	4 with tcos scheduler (cos with restarts) also adjusted lr to work with the batch size (learned from bs_9)	made a scheduler called tcos that does 10 epoch cosine cycles..  This was at epoch 46.  resubmitting!					
dg	r		bs_3_cos							"bs_3 with lr, bs, and scheduler set."	didn’t learn. Reducing lr to 0.0002 (maybe extra complexity is downward pressure on the lr?)					
dg			bs_4_cos				std_out_4_cos				"4 but with lr=0.0002, batch 10, cosine scheduler"					
dg	DONE!		bs_2_cos						0.09		"bs_2 had lr 0.0001 with cosine warmup.  This is cosine (no warmup) with lr 0.0002 and bs 10.  bs_2 looks to be learning slow there at the end, so yeah, this seemed like a good test"					
dg	DONE!		bs_3_tcos						0.926	3 with tcos scheduler.  						
dg			bs_10_tcos						0.89	python -m train_hydra pipeline=sc dataset=sc10 model=s4 model.n_layers=2 model.d_model=128 optimizer.lr=0.0001 loader.batch_size=5 loader.eval_resolutions=1 tc.conv.segment_length=400 tc.conv.stride=100 tc.transformer.trans_dim=100 tc.transformer.ff_dim=100 tc.transformer.num_heads=20 scheduler=tcos. didn't checkpoint it.  fixing.  	"same as 10, but with 100 stride (not 400).  This is a beefier transformer, with a larger input window.  Reduced dmodel to 128.  "	more!				
dg			bs_18						0.1	python -m train_hydra pipeline=sc dataset=sc10 model=s4 model.n_layers=2 model.d_model=128 scheduler=cosine	reduced dmodel was the big change.  Used cosine.  Switching to tcos					
dg			bs_2tcos						0.766	python -m train_hydra pipeline=sc dataset=sc10 optimizer.lr=0.0002 model=s4 model.n_layers=2 loader.eval_resolutions=1 loader.batch_size=10 scheduler=tcos tc.conv.stride=20	"scheduler tcos, and a stride of 20.  small batchsize to compensate.  10 hours gets you 25~ epochs. Still increasing! Running from ckpt"					
dg			bs_19tcos							python -m train_hydra pipeline=sc dataset=sc10 model=s4 model.n_layers=2 model.d_model=512 scheduler=tcos optimizer.lr=0.0002 loader.batch_size=10 loader.eval_resolutions=1 tc.conv.stride=20 tc.transformer.trans_dim=20 tc.transformer.ff_dim=100 tc.transformer.num_layers=10 tc.transformer.num_heads=5 	"beefy transformer like 4, and a shorter stride like 2, with tcos"					
dg			bs_19cos							"19, but with cosine warmup (like in 2_"						
dg			bs_20tcos							python -m train_hydra pipeline=sc dataset=sc10 model=s4 model.n_layers=2 model.d_model=512 scheduler=tcos optimizer.lr=0.0001 loader.batch_size=5 loader.eval_resolutions=1 tc.conv.segment_length=400 tc.conv.stride=20 tc.transformer.trans_dim=40 tc.transformer.ff_dim=100 tc.transformer.num_layers=10 tc.transformer.num_heads=10	"short stride, longer input segment, bigger transformer dimension, and more heads.  "					
dg			bs_20cos							"same as 20tcos, but with cosine warmup."						
dg			bs_6tcos							6 but with tcos! And reduced lr to 0.00002						
v			bs_21_cos							make one with more transformer heads.  Start with bs 5.  						
v			bs_21_tcos							exactly 20_cos but with tcos						
dg			bs_22	"these 4 are big boys.  But them on 4 a40s.  10 hrs, tcos3 (ask dallin)"						python -m train_hydra pipeline=sc dataset=sc10 model=s4 model.n_layers=2 model.d_model=512 loader.eval_resolutions=1 scheduler=tcos3 optimizer.lr=0.0002 loader.batch_size=10 hydra.run.dir=./outputs/p22/${now:%H-%M-%S-%f} tc.conv.segment_length=200 tc.conv.stride=20 tc.transformer.trans_dim=64 tc.transformer.ff_dim=128 tc.transformer.num_layers=16 tc.transformer.num_heads=16						
dg			bs_23							python -m train_hydra pipeline=sc dataset=sc10 model=s4 model.n_layers=4 model.d_model=512 loader.eval_resolutions=1 scheduler=tcos3 optimizer.lr=0.0002 loader.batch_size=10 hydra.run.dir=./outputs/p23/${now:%H-%M-%S-%f} tc.conv.segment_length=400 tc.conv.stride=20 tc.transformer.trans_dim=64 tc.transformer.ff_dim=128 tc.transformer.num_layers=16 tc.transformer.num_heads=16						
dg			bs_24							python -m train_hydra pipeline=sc dataset=sc10 model=s4 model.n_layers=6 model.d_model=512 loader.eval_resolutions=1 scheduler=tcos3 optimizer.lr=0.0001 loader.batch_size=5 hydra.run.dir=./outputs/p24/${now:%H-%M-%S-%f} tc.conv.segment_length=800 tc.conv.stride=20 tc.transformer.trans_dim=64 tc.transformer.ff_dim=128 tc.transformer.num_layers=16 tc.transformer.num_heads=16						
dg			bs_25							python -m train_hydra pipeline=sc dataset=sc10 model=s4 model.n_layers=6 model.d_model=512 loader.eval_resolutions=1 scheduler=tcos3 optimizer.lr=0.00005 loader.batch_size=1 hydra.run.dir=./outputs/p25/${now:%H-%M-%S-%f} tc.conv.segment_length=800 tc.conv.stride=10 tc.transformer.trans_dim=64 tc.transformer.ff_dim=128 tc.transformer.num_layers=16 tc.transformer.num_heads=8						
dg			bs_24tcos							same as 24 with tcos						
dg			bs_24tcos40							same as 24 with tcos40						